{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "landmark_seoul.py의 사본",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "label_df=pd.read_csv('/content/drive/MyDrive/data/train.csv')\n",
        "label_df.head()"
      ],
      "metadata": {
        "id": "gGMxiAUJmvGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def get_train_data(data_dir):\n",
        "  img_path_list=[]\n",
        "  label_list=[]\n",
        "\n",
        "  # get image path\n",
        "  img_path_list.extend(glob(os.path.join(data_dir,'*.PNG')))\n",
        "  img_path_list.sort(key=lambda x:int(x.split('/')[-1].split('.')[0]))\n",
        "\n",
        "  # get label\n",
        "  label_list.extend(label_df['label'])\n",
        "\n",
        "  return img_path_list,label_list\n",
        "\n",
        "def get_test_data(data_dir):\n",
        "  img_path_list=[]\n",
        "\n",
        "  # get image path\n",
        "  img_path_list.extend(glob(os.path.join(data_dir,'*.PNG')))\n",
        "  img_path_list.sort(key= lambda x:int(x.split('/')[-1].split('.')[0]))\n",
        "  print(img_path_list)\n",
        "\n",
        "  return img_path_list"
      ],
      "metadata": {
        "id": "Fjq-krPbm5-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_df.info())\n",
        "'''\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 723 entries, 0 to 722\n",
        "Data columns (total 2 columns):\n",
        " #   Column     Non-Null Count  Dtype \n",
        "---  ------     --------------  ----- \n",
        " 0   file_name  723 non-null    object\n",
        " 1   label      723 non-null    int64 \n",
        "dtypes: int64(1), object(1)\n",
        "memory usage: 11.4+ KB\n",
        "None\n",
        "'''"
      ],
      "metadata": {
        "id": "9K5frCeum9Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_img_path,all_label=get_train_data('/content/drive/MyDrive/data/train')\n",
        "test_img_path=get_test_data('/content/drive/MyDrive/data/test')"
      ],
      "metadata": {
        "id": "zSVHXPMinDUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_label[:5])\n",
        "print(all_img_path[:5])\n",
        "print(test_img_path[:5])"
      ],
      "metadata": {
        "id": "9AVrsjVDnP09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "dyOU5QkMnYUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU 체크 및 할당\n",
        "if torch.cuda.is_available():    \n",
        "    #device = torch.device(\"cuda:0\")\n",
        "    print('Device:', device)\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "metadata": {
        "id": "PmRVQ9vNndwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#하이퍼 파라미터 튜닝\n",
        "\n",
        "CFG = {\n",
        "    'IMG_SIZE':128, #이미지 사이즈\n",
        "    'EPOCHS':150, #에포크\n",
        "    'LEARNING_RATE':2e-2, #학습률\n",
        "    'BATCH_SIZE':32, #배치사이즈\n",
        "    'SEED':41, #시드\n",
        "}"
      ],
      "metadata": {
        "id": "ZlT_1laRnf5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed 고정\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED'])"
      ],
      "metadata": {
        "id": "-wCydGmIAoeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "metadata": {
        "id": "eaGEabSjnh32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,img_path_list,label_list,train_mode=True,transforms=None): #필요한 변수들을 선언\n",
        "    self.transforms=transforms\n",
        "    self.train_mode=train_mode\n",
        "    self.img_path_list=img_path_list\n",
        "    self.label_list=label_list\n",
        "\n",
        "  def __getitem__(self,index):  # index 번쨰 data를 return\n",
        "    img_path=self.img_path_list[index]\n",
        "    image=cv2.imread(img_path)\n",
        "    if self.transforms is not None:\n",
        "      image=self.transforms(image)\n",
        "\n",
        "    if self.train_mode:\n",
        "      label=self.label_list[index]\n",
        "      return image,label\n",
        "\n",
        "    else:\n",
        "      return image\n",
        "    \n",
        "  def __len__ (self):\n",
        "    return len(self.img_path_list)"
      ],
      "metadata": {
        "id": "WqyD0FmgnjjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempdataset=CustomDataset(all_img_path,all_label,train_mode=False)\n",
        "'''image = cv2.imread('/content/drive/MyDrive/data/train/001.PNG')\n",
        "plt.imshow(image)'''\n",
        "\n",
        "plt.imshow(tempdataset.__getitem__(88))"
      ],
      "metadata": {
        "id": "O9fBI6Z6nmqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tempdataset[20].shape)"
      ],
      "metadata": {
        "id": "11aYOI9LsWy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_len=(int(len(all_img_path) * 0.75))\n",
        "val_len=int(len(all_img_path) * 0.25)\n",
        "\n",
        "train_img_path=all_img_path[:train_len]\n",
        "train_label=all_label[:train_len]\n",
        "\n",
        "vali_img_path=all_img_path[train_len:]\n",
        "vali_label=all_label[train_len:]"
      ],
      "metadata": {
        "id": "vkBijyL5saKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_img_path))\n",
        "print(len(test_img_path))\n",
        "print(train_len)\n",
        "print(val_len)"
      ],
      "metadata": {
        "id": "wGQpaoArsb-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform=transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Numpy 배열에서 PIL 이미지로\n",
        "    transforms.Resize([CFG['IMG_SIZE'],CFG['IMG_SIZE']]), # 이미지 사이즈 변형\n",
        "    transforms.ToTensor(), # 이미지 데이터를 tensor\n",
        "    transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)), # 이미지 정규화\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomHorizontalFlip()\n",
        "    ])\n",
        "\n",
        "test_transform=transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize([CFG['IMG_SIZE'],CFG['IMG_SIZE']]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "WQUMbA-7sd5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Dataloader\n",
        "\n",
        "# CustomDataset class를 통하여 train dataset생성\n",
        "train_dataset=CustomDataset(train_img_path, train_label,train_mode=True,transforms=train_transform)\n",
        "# 만든 train dataset를 DataLoader에 넣어 batch 만들기\n",
        "train_loader=DataLoader(train_dataset,batch_size=CFG['BATCH_SIZE'],shuffle=True,num_workers=0)\n",
        "\n",
        "# validation도 적용\n",
        "vali_dataset=CustomDataset(vali_img_path,vali_label,train_mode=True,transforms=test_transform)\n",
        "vali_loader=DataLoader(vali_dataset,batch_size=CFG['BATCH_SIZE'],shuffle=False,num_workers=0)"
      ],
      "metadata": {
        "id": "pPTeOaqxsgJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches=len(train_loader)\n",
        "vali_batches=len(vali_loader)\n",
        "\n",
        "print('total train imgs:',train_len,'/ total train bathcnes:', train_batches)\n",
        "print('total valid imgs:',val_len,'/ total valid batches:', vali_batches)"
      ],
      "metadata": {
        "id": "0Wip9e9csh99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "resnet=models.resnet50(pretrained=True).to(device)\n",
        "for param in resnet.parameters():\n",
        "  param.requires_grad=False\n",
        "\n",
        "in_features=resnet.fc.in_features\n",
        "\n",
        "classifier=nn.Sequential(\n",
        "    nn.Linear(in_features,1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(1024,10),\n",
        "\n",
        ")\n",
        "\n",
        "resnet.fc=classifier\n",
        "\n",
        "\n",
        "criterion=torch.nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(params=resnet.parameters(),lr=CFG['LEARNING_RATE'])\n",
        "scheduler=None\n",
        "\n",
        "resnet.to(device)"
      ],
      "metadata": {
        "id": "pYdT381WAyqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(resnet,(3,128,128))"
      ],
      "metadata": {
        "id": "wvRG4gi-A3yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model,optimizer,train_loader,scheduler,device):\n",
        "  model.to(device)\n",
        "  n=len(train_loader)\n",
        "  best_loss=10000\n",
        "\n",
        "  for epoch in range(1,CFG['EPOCHS']+1):  # 에포크 설정\n",
        "    model.train()\n",
        "    running_loss=0.0\n",
        "\n",
        "    for img,label in tqdm(iter(train_loader)):\n",
        "      img,label=img.to(device),label.to(device)\n",
        "      optimizer.zero_grad() # 배치마다 optimzier 초기화\n",
        "\n",
        "      logit=model(img)\n",
        "      loss=criterion(logit,label)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step() # 가중치 최적화\n",
        "      running_loss+=loss.item()\n",
        "\n",
        "    print('[%d] Train loss: %.10f' %(epoch, running_loss/len(train_loader)))\n",
        "\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "\n",
        "    # validation set 평가\n",
        "    model.eval() # evaluatoin 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
        "    vali_loss=0.0\n",
        "    correct=0\n",
        "\n",
        "    with torch.no_grad(): # 파라미터 업데이트 안하기 때문에 no_grad 사용\n",
        "      for img,label in tqdm(iter(vali_loader)):\n",
        "        img,label=img.to(device),label.to(device)\n",
        "\n",
        "        logit=model(img)\n",
        "        vali_loss+= criterion(logit,label)\n",
        "        pred=logit.argmax(dim=1,keepdim=True)  # 10개의 class 중 가장 값이 높은 것을 예측 label로 추출\n",
        "        correct += pred.eq(label.view_as(pred)).sum().item()  # 예측값과 실제값이 맞으면 1 아니면 0으로 합산\n",
        "    vali_acc=100 * correct/len(vali_loader.dataset)\n",
        "\n",
        "    print('Vail set: Loss: {:.4f}, Accuracy: {}/{} ( {:.0f}%)\\n'.format(vali_loss / len(vali_loader), correct, len(vali_loader.dataset), 100 * correct / len(vali_loader.dataset)))\n",
        "    \n",
        "    if best_loss > vali_loss:\n",
        "      best_loss=vali_loss\n",
        "      torch.save(model.state_dict(),'/content/drive/MyDrive/data/save/best_model.pth')\n",
        "      print('Model Saved')"
      ],
      "metadata": {
        "id": "Ns7lIOgEsjwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim # 최적화 알고리즘들이 포함힘\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = resnet.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
        "scheduler = None"
      ],
      "metadata": {
        "id": "wg_3c2IaA-zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(resnet, optimizer, train_loader, scheduler, device)"
      ],
      "metadata": {
        "id": "icSHwiVJsr46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model,test_loader,device):\n",
        "  model.eval()\n",
        "  model_pred=[]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for img in tqdm(iter(test_loader)):\n",
        "      img=img.to(device)\n",
        "\n",
        "      pred_logit=model(img)\n",
        "      pred_logit=pred_logit.argmax(dim=1,keepdim=True).squeeze(1)\n",
        "      model_pred.extend(pred_logit.tolist())\n",
        "  return model_pred"
      ],
      "metadata": {
        "id": "a4gh6AusMs30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset=CustomDataset(test_img_path,None,train_mode=False,transforms=test_transform)\n",
        "test_loader=DataLoader(test_dataset,batch_size=CFG['BATCH_SIZE'],shuffle=False,num_workers=0)\n",
        "\n",
        "checkpoint=torch.load('/content/drive/MyDrive/data/save/best_model.pth')\n",
        "Predictor=resnet.to(device)\n",
        "Predictor.load_state_dict(checkpoint)\n",
        "\n",
        "preds=predict(Predictor,test_loader,device)\n",
        "preds[0:5]"
      ],
      "metadata": {
        "id": "iWHUvj-YMwZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yC4uN9xLfGnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission=pd.read_csv('/content/drive/MyDrive/data/sample_submission.csv')\n",
        "submission['label']=preds"
      ],
      "metadata": {
        "id": "IDAKq7VrNVSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/drive/MyDrive/data/save/submission_resnet50_2.csv',index=False)"
      ],
      "metadata": {
        "id": "hT3sksqbNacs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
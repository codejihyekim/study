from sklearn.datasets import load_boston
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
import time
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt

datasets = load_boston()
x = datasets.data
y = datasets.target

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=66)

model = Sequential()
model.add(Dense(100,input_dim=13))
model.add(Dense(100))
model.add(Dense(100))
model.add(Dense(100))
model.add(Dense(50))
model.add(Dense(10))
model.add(Dense(1))

#3. 컴파일, 훈련
model.compile(loss='mse', optimizer='adam')

es = EarlyStopping(monitor = 'val_loss', patience = 50, mode = 'min', verbose=1, restore_best_weights=True)
#과접합을 방지하기 위해서 epoch을 많이 돌린 후,특정 시점에서 멈추는 것, 그 특정시점을 어떻게 정하는 가
# patience은 정확도가 제일 높은 상태이면 설정해놓은 값 안에서 높은 상태보다 낮으면 그 때 정지

start= time.time()
hist = model.fit(x_train, y_train, epochs=1000, batch_size=1, validation_split=0.2, callbacks=[es])   #여기서 loss & val_loss의 결과가 나옴  #es를 호출할거닷!
end= time.time()- start

print("걸린시간: ", round(end,3), '초')  #소수 3까지만 출력

#4. 평가, 예측
model.fit(x_train, y_train, epochs=1000, batch_size=1, validation_split=0.2, callbacks=[es])

loss= model.evaluate(x_test, y_test)
print('loss: ', loss)

y_predict = model.predict(x_test)

r2 = r2_score(y_test, y_predict)
print('r2스코어 : ', r2)

plt.figure(figsize=(9,5))

#print("=========================")
#print(hist)
#print("=========================")
#print(hist.history)
#print("=========================")
#print(hist.history['loss'])
print("=========================")
print(hist.history['val_loss'])

plt.plot(hist.history['loss'], marker = '.', c='red', label = 'loss')
plt.plot(hist.history['val_loss'], marker = '.', c='blue', label = 'val_loss')
plt.grid() #격자표시
plt.title('loss')
plt.ylabel('loss') #y축
plt.xlabel('epoch') #x축
plt.legend(loc='upper right')
plt.show()

'''
hist = [110.55961608886719, 91.42996978759766, 83.72286987304688, 74.83761596679688, 169.27525329589844, 79.88710021972656, 75.00188446044922, 74.42396545410156, 89.91206359863281, 75.74637603759766, 95.39653015136719, 71.1338119506836, 82.65741729736328, 74.17973327636719, 66.9767074584961, 66.72775268554688, 63.3089599609375, 87.05359649658203, 86.2173843383789, 175.66761779785156, 59.433013916015625, 61.57650375366211, 54.8188362121582, 92.9878921508789, 71.65386962890625, 52.062355041503906, 77.3482437133789, 74.75997161865234, 63.53505325317383, 67.9991226196289, 64.04474639892578, 112.45127868652344, 54.220985412597656, 65.46000671386719, 85.77082061767578, 97.71770477294922, 48.927494049072266, 52.51692199707031, 60.42930221557617, 164.19918823242188, 84.56196594238281, 54.41946792602539, 
45.88080978393555, 49.24239730834961, 52.504154205322266, 90.96868896484375, 58.7979736328125, 45.3358268737793, 52.21847915649414, 50.972434997558594, 61.15776443481445, 71.78291320800781, 48.18745040893555, 47.818580627441406, 45.88965606689453, 43.23172378540039, 47.21265411376953, 46.00537109375, 48.017005920410156, 43.2554931640625, 50.96062469482422, 47.05446243286133, 43.59292984008789, 66.06684112548828, 41.62687301635742, 58.281028747558594, 65.24424743652344, 40.39378356933594, 49.514957427978516, 52.03109359741211, 42.86315155029297, 44.0914192199707, 40.98849105834961, 41.45287322998047, 60.204246520996094, 40.37825393676758, 38.98518371582031, 46.214664459228516, 42.092952728271484, 39.112060546875, 42.39961624145508, 44.247650146484375, 42.97220993041992, 39.92661666870117, 40.49329376220703, 49.76459884643555, 43.228702545166016, 38.13839340209961, 43.02812576293945, 53.07269287109375, 43.1961784362793, 41.84305191040039, 42.998756408691406, 53.38838195800781, 56.17827606201172, 41.75306701660156, 37.42426300048828, 36.77717971801758, 35.463138580322266, 40.510292053222656, 38.97705841064453, 37.826637268066406, 38.60856246948242, 37.805049896240234, 40.59672927856445, 39.487159729003906, 34.57946014404297, 36.00661849975586, 35.13513946533203, 
105.21756744384766, 38.216026306152344, 34.18460464477539, 41.54450988769531, 35.80575180053711, 60.219058990478516, 35.377864837646484, 39.47092819213867, 38.4958381652832, 34.84575653076172, 44.67049026489258, 41.1043815612793, 39.61094284057617, 33.00794982910156, 34.23617172241211, 44.309078216552734, 42.89912796020508, 33.39378356933594, 45.37177658081055, 42.238338470458984, 31.783105850219727, 34.261741638183594, 36.13938903808594, 49.39482879638672, 33.66742706298828, 34.74067306518555, 39.6131591796875, 40.26033020019531, 40.27974319458008, 38.48860549926758, 35.67579650878906, 39.14725875854492, 33.1617431640625, 35.13108444213867, 43.07115173339844, 35.84400939941406, 34.70254135131836, 58.84922409057617, 32.08217239379883, 36.77195358276367, 47.02286911010742, 38.9010009765625, 32.51441192626953, 36.364898681640625, 32.20562744140625, 40.6278076171875, 34.963134765625, 34.959144592285156, 32.66354751586914, 35.74855422973633, 32.34856414794922, 58.049598693847656, 43.36567687988281, 41.836177825927734, 41.73527145385742, 35.38185501098633, 39.58121871948242, 39.62208938598633, 43.804317474365234, 40.361881256103516, 40.345767974853516, 38.46137237548828, 56.19607162475586, 42.7314453125, 34.80752182006836, 36.42786407470703, 34.631752014160156, 30.92102813720703, 40.73478317260742, 34.87392807006836, 36.839908599853516, 36.328330993652344, 41.16350555419922, 32.564598083496094, 38.522987365722656, 35.827518463134766, 33.176326751708984, 37.24769973754883, 38.268348693847656, 36.098934173583984, 31.830032348632812, 31.79363441467285, 33.11875534057617, 31.986225128173828, 76.6270751953125, 35.5006217956543, 33.69518280029297, 33.21188735961914, 39.00490951538086, 33.160701751708984, 36.54718780517578, 38.09376525878906, 33.71057891845703, 35.26314163208008, 36.14800262451172, 32.07835388183594, 44.46574401855469, 33.0733528137207, 43.12371063232422, 41.324764251708984, 40.82426834106445, 34.87296676635742, 48.30916213989258, 44.846221923828125, 33.27253341674805, 35.68233108520508, 34.238929748535156, 37.01942443847656, 36.43698501586914, 31.39239501953125, 33.16007995605469, 39.883609771728516, 34.55878829956055, 36.855430603027344, 40.4527587890625, 39.373313903808594, 33.67424011230469, 
33.100608825683594]
restore_best_weights = 
True: training이 끝난 후, model의 weight를 monitor하고 있던 값이 가장 좋았을 때의 weight로 복원함
False: 마지막 training이 끝난 후의 weight로 복원 
Epoch 172/1000
315/323 [============================>.] - ETA: 0s - loss: 32.1848Restoring model weights from the end of the best epoch: 122.
323/323 [==============================] - 1s 3ms/step - loss: 31.6968 - val_loss: 39.7371
Epoch 172: early stopping
걸린시간:  195.348 초
loss:  16.705366134643555
r2스코어 :  0.8001342528779566
restore_best_weights를 사용할 시 최적의 weight값을 기록만 할 뿐 저장 기능은 없다!
#patience=50으로 잡음
EarlyStopping이 된 그 지점의 val_loss값이 최저점이 아니다. 최저점으로부터 patience준 값이 더 나가서 stop된 것..
따라서 val_loss의 최저점은 Earlystopping이 된 그 앞 시점이라고 볼 수 있다!
restore_best_weights를 사용하면 monitor(val_loss)하고 있던 값이 가장 좋았을 때의 weight로 복원함
'''